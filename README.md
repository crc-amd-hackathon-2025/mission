**### Make a fork or copy of this repo and fill in your team submission details! ###**

# AMD_Robotics_Hackathon_2025_[Project Name]

## Team Information

**Team:**
- ID: 1
- Name: CRC
- Members:
    - Cedric
    - Remy
    - Clement

**Summary:** Content Creator Assistant

Our work focuses on developing a robotic system that can assist content creators in recording videos: the robot is capable of grabbing a camera, then pointing it towards a specific target (*e.g.*, a person), focusing on it while maintaining a stable shot, and following the target as it moves. Moreover, the robot is controlled through voice commands, allowing for a hands-free experience. This system aims to enhance the content creation process by providing dynamic and adaptive camera work, enabling creators to focus on their performance without worrying about camera operation.

**TODO**: Images or video demonstrating your project

## Submission Details

### 1. Mission Description
Real world application: Content creation assistance (see summary above for details).

### 2. Creativity
- *What is novel or unique in your approach?* We combine voice command recognition with real-time object tracking to create a hands-free camera operation system.
- *Innovation in design, methodology, or application*: A content creator does not need to manually operate the camera, nor does he need to hire a cameraman, as the robot can autonomously handle camera positioning and tracking based on voice commands.

### 3. Technical implementations
- *Teleoperation / Dataset capture*:
<video controls width="640" height="360">
  <source src="assets/demo_teleop.webm" type="video/webm">
</video>
- *Training*: For full details about the training procedure, please refer to [the dedicated Jupyter notebook](https://github.com/crc-amd-hackathon-2025/mission/blob/main/mission/code/training-models-on-rocm.ipynb).
- *Inference*
    - **TODO**: Image/video of inference eval

### 4. Ease of use
- *How generalizable is your implementation across tasks or environments?*: The system can grab the camera from various positions (as long as it is within reach) and can track different types of targets (people, objects).
- *Flexibility and adaptability of the solution*: The voice command interface allows users to easily switch targets.
- *Types of commands or interfaces needed to control the robot*: Voice commands.

Besides, the YOLO-based tracking system requires an additional calibrations: we implemented an automatic calibration tool. The results are stored in a JSON configuration file that can easily be shared and is automatically loaded at restart. 

## Additional Links
*For example, you can provide links to:*

- *Link to a video of your robot performing the task*
- *URL of your dataset in Hugging Face*: https://huggingface.co/datasets/crc-amd-hackathon-2025/grab-cam
- *URL of your model in Hugging Face*: **TODO**: add link to the policy model

## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal
AMD_Robotics_Hackathon_2025_ProjectTemplate-main/
├── README.md
└── mission
    ├── code
    │   └── <code and script>
    └── wandb
        └── <latest run directory copied from wandb of your training job>
```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
├── debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
├── debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
├── latest-run -> run-20251029_063411-tz1cpo59
└── run-20251029_063411-tz1cpo59
    ├── files
    │   ├── config.yaml
    │   ├── output.log
    │   ├── requirements.txt
    │   ├── wandb-metadata.json
    │   └── wandb-summary.json
    ├── logs
    │   ├── debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    │   ├── debug-internal.log
    │   └── debug.log
    ├── run-tz1cpo59.wandb
    └── tmp
        └── code
```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.
